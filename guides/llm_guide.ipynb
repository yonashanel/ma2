{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AI & Machine Learning (KAN-CINTO4003U) - Copenhagen Business School | Spring 2025**\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"media/llm_header.png\" alt=\"LLM\" width=\"800\"/> <br>\n",
    "Image from Speech and Language Processing. Daniel Jurafsky & James H. Martin.<br> Copyright © 2024. All\n",
    "rights reserved. Draft of January 12, 2025.\n",
    "</p>\n",
    "\n",
    "\n",
    "# Generative LLMs: decoder-only Large Language Models\n",
    "\n",
    "***\n",
    "\n",
    "- **Large Language Models (LLMs)** are deep learning models trained on vast amounts of text data to understand, generate, and manipulate human language. They belong to the family of Transformer-based architectures, first introduced in the seminal paper *Attention Is All You Need* ([Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)), which revolutionized Natural Language Processing (NLP) through self-attention mechanisms.  \n",
    "\n",
    "- The development of LLMs gained momentum with OpenAI’s *Generative Pre-trained Transformer (GPT-1)* ([Radford et al., 2018](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)), which introduced a pretraining-finetuning paradigm for NLP tasks. This approach was later scaled with *GPT-2* ([Radford et al., 2019](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)) and *GPT-3* ([Brown et al., 2020](https://arxiv.org/abs/2005.14165)), with the latter demonstrating emergent capabilities in zero-shot and few-shot learning.  \n",
    "\n",
    "- **BERT** ([Devlin et al., 2018](https://arxiv.org/abs/1810.04805)) played a parallel but pivotal role in advancing Natural Language Understanding (NLU) with bidirectional context modeling. In particular, BERT influenced many retrieval-based applications. Meanwhile, GPT models prioritized *autoregressive decoding*, which revitalized the field of Natural Language Generation (NLG). The bifurcation between *encoder-only* (BERT-like) and *decoder-only* (GPT-like) architectures shapes the evolution of modern NLP systems to this day.  \n",
    "\n",
    "- **Scaling Laws** ([Kaplan et al., 2020](https://arxiv.org/abs/2001.08361)) demonstrated that model performance improves logarithmically with increased parameter count, data, and compute. This insight led to the training of massive LLMs, such as *GPT-4* ([OpenAI, 2023](https://openai.com/research/gpt-4)), *PaLM* ([Chowdhery et al., 2022](https://arxiv.org/abs/2204.02311)), *Claude* ([Anthropic, 2023](https://www.anthropic.com)), and *Gemini* ([Google DeepMind, 2023](https://blog.google/technology/ai/google-gemini-ai/)), which exhibit improved reasoning, multilingual capabilities, and tool use.  \n",
    "\n",
    "- LLMs have been widely adopted in industry, powering chatbots (e.g., *ChatGPT*, *Claude*), code generation tools (e.g., *GitHub Copilot* powered by OpenAI’s *Codex* ([Chen et al., 2021](https://arxiv.org/abs/2107.03374))), and enterprise applications. For example, [Morgan Stanley](https://www.morganstanley.com/articles/morgan-stanley-ai-assistant?utm_source=chatgpt.com) uses LLMs to retrieve financial insights, while [Salesforce](https://www.salesforce.com/news/stories/how-salesforce-ai-assistant-works/?utm_source=chatgpt.com) has integrated LLMs into its AI assistant for customer relationship management (CRM).  \n",
    "\n",
    "- Despite their strengths, LLMs have significant limitations: They are compute-intensive, struggle with factual consistency ([Maynez et al., 2020](https://arxiv.org/abs/2010.03043)), and often hallucinate incorrect information. Efforts to mitigate these issues include RLHF (Reinforcement Learning from Human Feedback) ([Christiano et al., 2017](https://arxiv.org/abs/1706.03741)) and retrieval-augmented generation (RAG) ([Lewis et al., 2020](https://arxiv.org/abs/2005.11401)), which combines LLMs with external knowledge sources.  \n",
    "\n",
    "***\n",
    "\n",
    "<br><br>\n",
    "\n",
    "**In this notebook, we will explore how to use generative LLMs via WatsonX.AI.**\n",
    "\n",
    "<br><br>\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction to decoder-only LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:rgba(4, 12, 78, 0.58); color: #ffffff; font-weight: 700; padding-left: 10px; padding-top: 20px; padding-bottom: 20px\"><strong>Generative AI models</strong></div>\n",
    "\n",
    "<div style=\"background-color:rgb(13, 14, 18); padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
    "\n",
    "<div style=\"padding-left: 10px; padding-right: 10px; padding-top: 10px; padding-bottom: 30px, align: justify\">\n",
    "<p align=\"center\">\n",
    "<img src=\"media/autoregressive.png\" alt=\"autoregressive\" width=\"800\"/>\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:rgb(13, 14, 18); padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px; color: white;\">\n",
    "<p>\n",
    "As mentioned above, the rise of Large Language Models (LLMs) is a direct result of advancements in Transformer-based neural network architectures, originally introduced in <i>Attention Is All You Need</i> (<a href=\"https://arxiv.org/abs/1706.03762\">Vaswani et al., 2017</a>). <br><br>While early Transformer models were primarily used for sequence-to-sequence tasks (such as machine translation), modern LLMs - so-called decoder-only LLMs - are capable of human-level natural language generation (NLG) and, arguably, near-human reasoning, dialogue, and tool usage.\n",
    "\n",
    "Unlike earlier neural architectures like convolutional neural networks (CNNs), recurrent neural networks (RNNs), or long short-term memory (LSTM) networks, Transformers (and by extension, decoder-only LLMs) rely on self-attention mechanisms to dynamically weigh relationships between words in a sequence. Effectively, this means that instead of reading text one word at a time in order (like RNNs) or trying to spot patterns in small chunks (like CNNs), Transformers look at the entire sentence at once and decide which words are most important to each other.\n",
    "\n",
    "**For example**: Imagine you're reading a mystery novel. Instead of going page by page and only remembering the last few sentences, a Transformer can scan the whole book at once and instantly see connections—like how a clue in Chapter 2 relates to the big reveal in Chapter 20. Much like what humans do when reading a book, Transformers can understand context and meaning across long and complex sentences.\n",
    "\n",
    "This ability to \"pay attention\" to all words at once makes LLMs much better at understanding context and meaning, even across long and complex sentences. This, however, has it limits.\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "<div style=\"background-color:rgba(4, 12, 78, 0.58); color: #ffffff; font-weight: 700; padding-left: 10px; padding-top: 20px; padding-bottom: 20px\">\n",
    "    <strong>Key aspects governing LLM behavior</strong>\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color:rgb(13, 14, 18); padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px; color: white;\">\n",
    "\n",
    "<ul>\n",
    "    <li><strong>Context Window</strong><br>\n",
    "        Defines how many tokens (words, subwords, or characters) the model can consider at once. Affects how well it retains information in long conversations or documents.\n",
    "        <br><br>\n",
    "    </li>\n",
    "    <li><strong>Temperature</strong><br>\n",
    "        Controls randomness in text generation. Lower values (e.g., 0.1) make responses more deterministic, while higher values (e.g., 1.0) make them more creative.\n",
    "        <br><br>\n",
    "    </li>\n",
    "    <li><strong>Top-k Sampling</strong><br>\n",
    "        Limits the model to choosing from the <i>k</i> most likely next tokens instead of considering all possibilities, reducing extreme randomness.\n",
    "        <br><br>\n",
    "    </li>\n",
    "    <li><strong>Top-p (Nucleus) Sampling</strong><br>\n",
    "        Selects tokens from the smallest possible set whose cumulative probability exceeds <i>p</i>. More flexible than top-k, ensuring balanced randomness.\n",
    "        <br><br>\n",
    "    </li>\n",
    "    <li><strong>Repetition Penalty</strong><br>\n",
    "        Adjusts the likelihood of repeating words or phrases. Helps prevent redundant or looping responses.\n",
    "        <br><br>\n",
    "    </li>\n",
    "    <li><strong>Stop Sequences</strong><br>\n",
    "        Predefined words or phrases that signal the model to stop generating text, useful for structuring responses.\n",
    "        <br><br>\n",
    "    </li>\n",
    "    <li><strong>System and User Prompts</strong><br>\n",
    "        The initial instructions given to the model (system prompt) and user inputs. Strongly influences the model's behavior and output style.\n",
    "        <br><br>\n",
    "    </li>\n",
    "    <li><strong>Memory and Retrieval</strong><br>\n",
    "        Some models have long-term memory or external retrieval systems (e.g., RAG - Retrieval-Augmented Generation) to pull in relevant context beyond the context window.\n",
    "        <br><br>\n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Connecting to WatsonX.ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you haven't already, please follow the WatsonX.ai Guide on Canvas to set up your account and get your API key. You will need this key to connect to the WatsonX.ai API.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decouple import config\n",
    "from ibm_watsonx_ai import APIClient\n",
    "from ibm_watsonx_ai import Credentials\n",
    "from ibm_watsonx_ai.foundation_models import ModelInference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Loading your API key\n",
    "You can use your IBM Cloud API key in one of two ways\n",
    "\n",
    "- Using `getpass` to enter your API key securely in the notebook (has to be done every time you restart the notebook)\n",
    "- Storing your API key in a .env file and loading it using the `python-decouple` package (recommended)\n",
    "\n",
    "**DO NOT HARD-CODE YOUR API KEY IN THE NOTEBOOK. EVER!!**\n",
    "\n",
    "**IF YOU STORE YOUR API KEY IN THE .env FILE - MAKE SURE IT .env IS ALSO IN .gitignore SO IT IS NOT COMMITED TO YOUR REMOTE GH REPO!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefinedValueError",
     "evalue": "WX_API_KEY not found. Declare it as envvar or define a default value.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUndefinedValueError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load the environment variables using python-decouple\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# The .env file should be in the root of the project\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# The .env file should NOT be committed to the repository\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m WX_API_KEY = \u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mWX_API_KEY\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\wwwkr\\anaconda3\\envs\\aiml25-ma2\\Lib\\site-packages\\decouple.py:248\u001b[39m, in \u001b[36mAutoConfig.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    245\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config:\n\u001b[32m    246\u001b[39m     \u001b[38;5;28mself\u001b[39m._load(\u001b[38;5;28mself\u001b[39m.search_path \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._caller_path())\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\wwwkr\\anaconda3\\envs\\aiml25-ma2\\Lib\\site-packages\\decouple.py:107\u001b[39m, in \u001b[36mConfig.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m    104\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    105\u001b[39m \u001b[33;03m    Convenient shortcut to get.\u001b[39;00m\n\u001b[32m    106\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\wwwkr\\anaconda3\\envs\\aiml25-ma2\\Lib\\site-packages\\decouple.py:92\u001b[39m, in \u001b[36mConfig.get\u001b[39m\u001b[34m(self, option, default, cast)\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     91\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(default, Undefined):\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m UndefinedValueError(\u001b[33m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m not found. Declare it as envvar or define a default value.\u001b[39m\u001b[33m'\u001b[39m.format(option))\n\u001b[32m     94\u001b[39m     value = default\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(cast, Undefined):\n",
      "\u001b[31mUndefinedValueError\u001b[39m: WX_API_KEY not found. Declare it as envvar or define a default value."
     ]
    }
   ],
   "source": [
    "# Load the environment variables using python-decouple\n",
    "\n",
    "# The .env file should be in the root of the project\n",
    "# The .env file should NOT be committed to the repository\n",
    "\n",
    "WX_API_KEY = config('WX_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Connecting to the WatsonX.ai Credentials API\n",
    "\n",
    "To authenticate and call LLMs via WatsonX.ai we need\n",
    "\n",
    "* The URL of the WatsonX.ai API\n",
    "* Your unique API key that you created in the WatsonX.ai guide\n",
    "* The unique project ID of the project you created in the WatsonX.ai guide.\n",
    "* The model ID of the LLM you want to use\n",
    "\n",
    "**NOTE**: Depending on which region you picked for your project, you need to use the corresponding URL:\n",
    "\n",
    "* Dallas: https://us-south.ml.cloud.ibm.com\n",
    "* Frankfurt: https://eu-de.ml.cloud.ibm.com\n",
    "* London: https://eu-gb.ml.cloud.ibm.com\n",
    "* Sydney: https://au-syd.ml.cloud.ibm.com\n",
    "* Tokyo: https://jp-tok.ml.cloud.ibm.com\n",
    "* Toronto: https://ca-tor.ml.cloud.ibm.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials = Credentials(\n",
    "    url = \"https://eu-de.ml.cloud.ibm.com\",\n",
    "    api_key = WX_API_KEY\n",
    ")\n",
    "\n",
    "client = APIClient(\n",
    "    credentials=credentials, \n",
    "    project_id=\"ea176fc8-4852-4798-bf23-063620807ec9\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Testing the connection\n",
    "\n",
    "The `ModelInference` class is a wrapper around the WatsonX.ai API. It allows you to interact with the LLMs via the API. If we were to look inside the `ModelInference` class, we would see that it uses the `requests` library to make HTTP requests to the server on IBM Cloud that hosts the WatsonX.ai API and the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = ModelInference(\n",
    "    api_client=client,\n",
    "    model_id=\"ibm/granite-13b-instruct-v2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_id': 'ibm/granite-13b-instruct-v2',\n",
       " 'created_at': '2025-03-01T23:38:27.533Z',\n",
       " 'results': [{'generated_text': 'Mix the ingredients together in a bowl. Pour the batter into a cake pan. Bake for 30 minutes',\n",
       "   'generated_token_count': 20,\n",
       "   'input_token_count': 7,\n",
       "   'stop_reason': 'max_tokens'}],\n",
       " 'system': {'warnings': [{'message': \"The value of 'parameters.max_new_tokens' for this model was set to value 20\",\n",
       "    'id': 'unspecified_max_new_tokens',\n",
       "    'additional_properties': {'limit': 0,\n",
       "     'new_value': 20,\n",
       "     'parameter': 'parameters.max_new_tokens',\n",
       "     'value': 0}}]}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"How do I make a cake?\"\n",
    "generated_response = model.generate(prompt)\n",
    "\n",
    "generated_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we get a warning that \"*The value of `parameters.max_new_tokens` for this model was set to value 20*\". This means that the model will generate a maximum of 20 tokens (words, subwords, or characters) in response to each prompt. This is a safety measure to prevent the model from generating too much text at once. You can adjust this value as needed, but be aware that generating large amounts of text can be computationally expensive and may take longer to complete.\n",
    "\n",
    "Notice also that the `stop_reason` was 'max_tokens', which means that the model stopped generating text because it reached the maximum number of tokens allowed. This is expected behavior and is not an error.\n",
    "\n",
    "You can check which parameters can be set like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| PARAMETER             | TYPE                                   | EXAMPLE VALUE                                                                                                                             |\n",
      "+=======================+========================================+===========================================================================================================================================+\n",
      "| decoding_method       | str, TextGenDecodingMethod, NoneType   | sample                                                                                                                                    |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| length_penalty        | dict, TextGenLengthPenalty, NoneType   | {'decay_factor': 2.5, 'start_index': 5}                                                                                                   |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| temperature           | float, NoneType                        | 0.5                                                                                                                                       |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| top_p                 | float, NoneType                        | 0.2                                                                                                                                       |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| top_k                 | int, NoneType                          | 1                                                                                                                                         |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| random_seed           | int, NoneType                          | 33                                                                                                                                        |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| repetition_penalty    | float, NoneType                        | 2                                                                                                                                         |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| min_new_tokens        | int, NoneType                          | 50                                                                                                                                        |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| max_new_tokens        | int, NoneType                          | 1000                                                                                                                                      |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| stop_sequences        | list, NoneType                         | 200                                                                                                                                       |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| time_limit            | int, NoneType                          | 600000                                                                                                                                    |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| truncate_input_tokens | int, NoneType                          | 200                                                                                                                                       |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| return_options        | dict, ReturnOptionProperties, NoneType | {'input_text': True, 'generated_tokens': True, 'input_tokens': True, 'token_logprobs': True, 'token_ranks': False, 'top_n_tokens': False} |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| include_stop_sequence | bool, NoneType                         | True                                                                                                                                      |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| prompt_variables      | dict, NoneType                         | {'doc_type': 'emails', 'entity_name': 'Golden Retail'}                                                                                    |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "from ibm_watsonx_ai.foundation_models.schema import TextGenParameters\n",
    "\n",
    "TextGenParameters.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Setting parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = TextGenParameters(\n",
    "    temperature=0.8,      # Higher temperature means more randomness\n",
    "    max_new_tokens=500, # Maximum number of tokens to generate\n",
    "    min_new_tokens=200, # Minimum number of tokens to generate\n",
    ")\n",
    "\n",
    "model = ModelInference(\n",
    "    api_client=client,\n",
    "    model_id=\"ibm/granite-13b-instruct-v2\",\n",
    "    params=PARAMS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_id': 'ibm/granite-13b-instruct-v2',\n",
       " 'created_at': '2025-03-01T23:39:30.770Z',\n",
       " 'results': [{'generated_text': 'You can make a cake by following a recipe. You can also make a cake by following a mix. You can also make a cake by following a box. You can also make a cake by following a box and adding your own touches. You can also make a cake by following a recipe and adding your own touches. \\nYou can also make a cake by following a mix and adding your own touches. You can also make a cake by following a box and adding your own touches. You can also make a cake by following a recipe and adding your own touches. You can also make a cake by following a mix and adding your own touches. You can also make a cake by following a box and adding your own touches. \\nYou can also make a cake by following a recipe and adding your own touches. You can also make a cake by following a mix and adding your own touches. You can also make a cake by following a box and adding your own touches. \\nYou can also make a cake by following a recipe and adding your own touches. You can also make a cake by following a mix and adding your own touches. You',\n",
       "   'generated_token_count': 232,\n",
       "   'input_token_count': 7,\n",
       "   'stop_reason': 'eos_token',\n",
       "   'seed': 3987455811}]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = model.generate(prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can make a cake by following a recipe. You can also make a cake by following a mix. You can also make a cake by following a box. You can also make a cake by following a box and adding your own touches. You can also make a cake by following a recipe and adding your own touches. \n",
      "You can also make a cake by following a mix and adding your own touches. You can also make a cake by following a box and adding your own touches. You can also make a cake by following a recipe and adding your own touches. You can also make a cake by following a mix and adding your own touches. You can also make a cake by following a box and adding your own touches. \n",
      "You can also make a cake by following a recipe and adding your own touches. You can also make a cake by following a mix and adding your own touches. You can also make a cake by following a box and adding your own touches. \n",
      "You can also make a cake by following a recipe and adding your own touches. You can also make a cake by following a mix and adding your own touches. You\n"
     ]
    }
   ],
   "source": [
    "print(response[\"results\"][0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Using LLMs for classification\n",
    "\n",
    "In this section, we will use a decoder-only LLM to classify text. We will provide the model with a prompt and ask it to generate a response that classifies the text into one of several categories. This is a common use case for LLMs in NLP, where they can be used to classify text based on the context and content of the input.\n",
    "\n",
    "Note that what we provide here is a very \"vanilla\" example of prompt engineering and optimization. In practice, you would likely need to experiment with different prompts and parameters to get the best results for your specific use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report \n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0. Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be working with the AG News dataset, which is a collection of news articles from the AG's corpus of news articles on the web. The dataset has four categories: World, Sports, Business, and Science/Technology. We will use a subset of the dataset for this example.\n",
    "\n",
    "We can load this data directly from [Hugging Face Datasets](https://huggingface.co/docs/datasets/) - The HuggingFace Hub- into a Pandas DataFrame. Pretty neat!\n",
    "\n",
    "**Note**: This cell will download the dataset and keep it in memory. If you run this cell multiple times, it will download the dataset multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "splits = {'train': 'data/train-00000-of-00001.parquet', 'test': 'data/test-00000-of-00001.parquet'}\n",
    "# train = pd.read_parquet(\"hf://datasets/fancyzhx/ag_news/\" + splits[\"train\"])\n",
    "test = pd.read_parquet(\"hf://datasets/fancyzhx/ag_news/\" + splits[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((760, 2),)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "label_map = {\n",
    "    0: 'World',\n",
    "    1: 'Sports',\n",
    "    2: 'Business',\n",
    "    3: 'Sci/Tech'\n",
    "}\n",
    "\n",
    "def preprocess(df: pd.DataFrame, frac : float = 1e-2, label_map : dict[int, str] = label_map, seed : int = 42) -> pd.DataFrame:\n",
    "    \"\"\" Preprocess the dataset \n",
    "\n",
    "    Operations:\n",
    "    - Map the label to the corresponding category\n",
    "    - Filter out the labels not in the label_map\n",
    "    - Sample a fraction of the dataset (stratified by label)\n",
    "\n",
    "    Args:\n",
    "    - df (pd.DataFrame): The dataset to preprocess\n",
    "    - frac (float): The fraction of the dataset to sample in each category\n",
    "    - label_map (dict): A mapping of the original label to the new label\n",
    "    - seed (int): The random seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The preprocessed dataset\n",
    "    \"\"\"\n",
    "\n",
    "    return  (\n",
    "        df\n",
    "        .assign(label=lambda x: x['label'].map(label_map))\n",
    "        [lambda df: df['label'].isin(label_map.values())]\n",
    "        .groupby('label')[[\"text\", \"label\"]]\n",
    "        .apply(lambda x: x.sample(frac=frac, random_state=seed))\n",
    "        .reset_index(drop=True)\n",
    "\n",
    "    )\n",
    "\n",
    "# train_df = preprocess(train, frac=0.01)\n",
    "test_df = preprocess(test, frac=0.1)\n",
    "\n",
    "# clear up some memory by deleting the original dataframes\n",
    "# del train\n",
    "del test\n",
    "\n",
    "test_df.shape # , train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Split data\n",
    "\n",
    "Notice that we don't need any training data for LLMs! This is one of the key advantages of using pre-trained models. The model has already been trained on a large corpus of text data and has learned to generate text based on that training. We can use the model directly for text classification without needing to train it on any specific data.\n",
    "\n",
    "**However**, it is good practice to set a side a small portion of your data to work with while you are developing your model. We still want our LLM system to generalize!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Set model parameers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = TextGenParameters(\n",
    "    temperature=0,              # Higher temperature means more randomness - In this case we don't want randomness\n",
    "    max_new_tokens=10,          # Maximum number of tokens to generate\n",
    "    stop_sequences=[\".\", \"\\n\"], # Stop generating text when these sequences are encountered\n",
    ")\n",
    "\n",
    "model = ModelInference(\n",
    "    api_client=client,\n",
    "    model_id=\"ibm/granite-13b-instruct-v2\",  # We could also try a larger model!\n",
    "    params=PARAMS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Create a system prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You task is to classify news stories into one of five categories\n",
    "\n",
    "CATEGORIES:\n",
    "{categories}\n",
    "\n",
    "TEXT:\n",
    "{text}\n",
    "\n",
    "Please assign the correct category to the text. Answer with the correct category and nothing else.\n",
    "\n",
    "Category:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Generate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 760/760 [01:47<00:00,  7.06it/s]\n"
     ]
    }
   ],
   "source": [
    "CATEGORIES = \"- \" + \"\\n- \".join(test_df[\"label\"].unique())  # Create a string with all categories\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for text in tqdm(test_df[\"text\"]):\n",
    "\n",
    "    # format the prompt with the categories and the text\n",
    "    prompt = SYSTEM_PROMPT.format(categories=CATEGORIES, text=text)\n",
    "    \n",
    "    # generate the response from the model\n",
    "    response = model.generate(prompt)\n",
    "\n",
    "    # extract the generated text from the response\n",
    "    prediction = response[\"results\"][0][\"generated_text\"].strip()\n",
    "\n",
    "    # append the prediction to the list of predictions\n",
    "    predictions.append(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Business       0.54      0.91      0.68       190\n",
      "    Sci/Tech       0.89      0.35      0.50       190\n",
      "      Sports       0.96      0.91      0.94       190\n",
      "       World       0.80      0.78      0.79       190\n",
      "\n",
      "    accuracy                           0.74       760\n",
      "   macro avg       0.80      0.74      0.73       760\n",
      "weighted avg       0.80      0.74      0.73       760\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_df.label, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml25-ma2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
