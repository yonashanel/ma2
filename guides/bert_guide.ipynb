{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AI & Machine Learning (KAN-CINTO4003U) - Copenhagen Business School | Spring 2025**\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"media/bert_header.jpg\" alt=\"BERT\" width=\"500\"/>\n",
    "</p>\n",
    "\n",
    "\n",
    "# BERT: Bidirectional Encoder Representations from Transformers\n",
    "\n",
    "***\n",
    "\n",
    "* Bidirectional Encoder Representations from Transformers (BERT) ([Devlin et al., 2018](https://arxiv.org/abs/1810.04805)) is a deep learning model developed by Google AI Language that significantly advanced Natural Language Processing (NLP), particularly in Natural Language Understanding (NLU). <br><br>\n",
    "* Many subsequent models, such as RoBERTa ([Liu et al., 2019](https://arxiv.org/abs/1907.11692)), ALBERT ([Lan et al., 2019](https://arxiv.org/abs/1909.11942)), and DistilBERT ([Sanh et al., 2019](https://arxiv.org/abs/1910.01108)), have built upon BERT’s architecture, improving efficiency and performance.<br><br>\n",
    "* The original BERT model was introduced in 2018, following OpenAI’s Generative Pre-trained Transformer (GPT-1) ([Radford et al., 2018](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)). Both models were based on the Transformer architecture (Vaswani et al., 2017), but they took different approaches: while GPT is a unidirectional model designed for Natural Language Generation (NLG), BERT introduced bidirectional self-attention to improve contextual understanding in NLU tasks. <br><br>\n",
    "* These two architectures played a pivotal role in modern NLP, with BERT influencing retrieval-based models and GPT evolving into more advanced generative AI systems such as the breakthrough of GPT-3 ([Brown et al., 2020](https://arxiv.org/abs/2005.14165)) and ChatGPT.<br><br>\n",
    "* BERT has seen wide industry applications. For example, Google [integrated BERT into its search algorithms](https://snorkel.ai/large-language-models/bert-models/?utm_source=chatgpt.com) to better understand user queries, leading to more accurate and contextually relevant search results. Other companies, [like Wayfair](https://www.aboutwayfair.com/tech-innovation/bert-does-business-implementing-the-bert-model-for-natural-language-processing-at-wayfair?utm_source=chatgpt.com), have implemented BERT to analyze customer messages, enabling more efficient and accurate responses. <br><br>\n",
    "* While highly effective for Natural Language Understanding (NLU), BERT is computationally expensive, limited to a 512-token context window, lacks generative capabilities, and inherits biases from its pretraining data, making it less suitable for real-time, long-document, or dynamically evolving knowledge tasks. Perfect for some tasks, but not all.<br><br>\n",
    "* In December 2024, [ModernBERT](https://huggingface.co/papers/2412.13663) was introduced as a state-of-the-art encoder-only model, offering significant improvements over previous architectures. It supports sequences up to 8,192 tokens and incorporates modern enhancements like Rotary Positional Embeddings (RoPE) and Flash Attention for improved performance and efficiency..<br><br>\n",
    "\n",
    "***\n",
    "\n",
    "<br><br>\n",
    "\n",
    "**In this notebook, we will explore the BERT model, its architecture and explain how to use ModernBERT for various NLP tasks using the Hugging Face Transformers library.**\n",
    "\n",
    "(Part is is loosely adopted from \"[A Complete Guide to BERT with Code](https://towardsdatascience.com/a-complete-guide-to-bert-with-code-9f87602e4a11/)\" (2024) by Bradney Smith and [github.com/AnswerDotAI/ModernBERT](https://github.com/AnswerDotAI/ModernBERT/blob/main/examples/finetune_modernbert_on_glue.ipynb))\n",
    "\n",
    "<br><br>\n",
    "\n",
    "**Note**: If your run into issues with memory or performance, consider using a GPU or a cloud-based service like Google Colab, which offers free GPU access. Apart from the nice header image, there are not external, local dependencies for this notebook, which means you can run it on any machine with Python installed. However, you still need to install the relevant packages, which are listed in `environment.yml`.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction to Transformers\n",
    "\n",
    "In 2017, the Transformer architecture revolutionized natural language processing (NLP) with the publication of the paper \"*Attention Is All You Need*\" by Vaswani et al.  Unlike older neural network models for language tasks - such as convolutional neural networks (CNNs), recurrent neural networks (RNNs) or long short-term memory networks (LSTMs) - the Transformer relies heavily on a mechanism known as **self-attention**. This approach allows the model to focus on different parts of a sentence (or sequence) when encoding its meaning, which was found to drastically improve both training efficiency and performance on large-scale language tasks.\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"background-color:rgba(4, 12, 78, 0.58); color: #ffffff; font-weight: 700; padding-left: 10px; padding-top: 20px; padding-bottom: 20px\"><strong>The original transformer</strong></div>\n",
    "\n",
    "<div style=\"background-color:rgb(13, 14, 18); padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
    "\n",
    "<div style=\"padding-left: 10px; padding-right: 10px; padding-top: 10px; padding-bottom: 30px, align: justify\">\n",
    "<p align=\"center\">\n",
    "<img src=\"media/transformer_architecture.png\" alt=\"Transformer Architecture\" width=\"800\"/>\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "<p>The original Transformer was designed as a so-called <i>encoder-decoder</i> model, primarily for machine translation. Here’s how it works in simple terms:</p>\n",
    "<ul>\n",
    "    <li><strong>Encoder<br><br></strong>\n",
    "        <ul>\n",
    "            <li>Converts (or “encodes”) an input sequence (e.g., a sentence in French) into a set of hidden, contextualized vector representations (hidden state as discussed class 2!), what is often referred to as “contextual embeddings”.<br><br></li>\n",
    "            <li>Multiple encoder layers apply self-attention to the input tokens. Self-attention means each token can “attend” to all other tokens, learning how each word relates to the rest of the sentence.<br><br></li>\n",
    "            <li>The encoder produces a context-rich representation of each token, capturing not just its meaning in isolation but its meaning relative to other words in the sequence.<br><br></li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><strong>Decoder<br><br></strong>\n",
    "        <ul>\n",
    "            <li>Generates (or “decodes”) an output sequence (e.g., the equivalent sentence in English) based on the encoder’s output.<br><br></li>\n",
    "            <li>Multiple decoder layers take two inputs:\n",
    "                <ul>\n",
    "                    <li>The representations produced by the encoder.</li>\n",
    "                    <li>A partial sequence of already generated tokens (so the decoder can attend to what it has produced so far).</li>\n",
    "                </ul>\n",
    "            <br></li>\n",
    "            <li>The decoder produces one token at a time, using both the encoder’s context and its own previously generated tokens to create a coherent output sequence.<br><br></li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "</div>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "Because the original Transformer was so successful at translation - an area that requires both deep semantic understanding and fluent generation - researchers quickly realized that the core ideas could be adapted for all sorts of NLP tasks. This led to three major “families” of Transformer-based architectures:\n",
    "\n",
    "\n",
    "1. **Encoder-Only models** (e.g., BERT series)\n",
    "    * Focuses on understanding an input sequence deeply, e.g., mapping the semantic vector representations of each token. Commonly used for classification, question answering, named entity recognition, and other analysis-driven tasks.<br><br>\n",
    "\n",
    "2. **Decoder-Only models** (e.g., GPT series)\n",
    "    * Focuses on generating or predicting the next tokens in a sequence, typically autoregressively. Commonly used for text generation, chatbots, and creative writing.<br><br>\n",
    "\n",
    "3. **Encoder-Decoder models** (e.g., T5)\n",
    "    * Focuses on bombining both understanding and generation of sequences, such as translation, summarization, and other tasks that require both comprehension and generation.<br><br>\n",
    "\n",
    "\n",
    "<div style=\"padding-left: 10px; padding-right: 10px; padding-top: 10px; padding-bottom: 30px, align: justify\">\n",
    "<p align=\"center\">\n",
    "<img src=\"media/transformer_families.webp\" alt=\"Transformer Architecture\" width=\"800\"/>\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. BERT Architecture\n",
    "\n",
    "Before we dive into the BERT architecture, let's first understand the building blocks and concepts that make up the model:\n",
    "\n",
    "\n",
    "<div style=\"background-color:rgba(4, 12, 78, 0.58); color: #ffffff; font-weight: 700; padding-left: 10px; padding-top: 20px; padding-bottom: 20px\"><strong>To understand BERT, it helps to understand a few key concepts and components</strong></div>\n",
    "\n",
    "<div style=\"background-color:rgb(13, 14, 18); padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
    "\n",
    "<div style=\"padding-left: 10px; padding-right: 10px; padding-top: 10px; padding-bottom: 30px, align: justify\">\n",
    "</div>\n",
    "\n",
    "<ul>\n",
    "    <li><strong>Tokens and Tokenization<br><br></strong>\n",
    "        <ul>\n",
    "            <li><strong>Token:</strong> A token is a basic unit of text, which could be a word, subword, or character. In BERT, tokens are the smallest units that the model processes.<br><br></li>\n",
    "            <li><strong>Tokenization:</strong> The process of converting raw text into tokens. BERT uses WordPiece tokenization, which breaks down words into subwords or characters to handle out-of-vocabulary words.<br><br></li>\n",
    "            <div align=\"center\"><img src=\"media/tokenization_BERT.png\" alt=\"Tokenization BERT\" width=\"800\"/><br><br></li></div>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><strong>Special Tokens<br><br></strong>\n",
    "        <ul>\n",
    "            <li><strong>[CLS]:</strong> A special token added at the beginning of each input sequence. The final hidden state corresponding to this token is used as the aggregate sequence representation for classification tasks. In short, the final hidden representation of the special CLS token often serves as the “summary” vector for classification tasks.<br><br></li>\n",
    "            <li><strong>[SEP]:</strong> A special token used to separate different sentences in a single input sequence. It helps the model distinguish between different segments.<br><br></li>\n",
    "            <div align=\"center\"><img src=\"media/cls_token_bert.png\" alt=\"Special Tokens BERT\" width=\"800\"/><br><br></li></div>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><strong>Context Length<br><br></strong>\n",
    "        <ul>\n",
    "            <li>BERT can handle input sequences up to 512 tokens in length. (ModernBERT can handle 8K+)<br><br></li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><strong>Attention Mechanism<br><br></strong>\n",
    "        <ul>\n",
    "            <li>BERT uses self-attention to compute a representation of the input sequence. Each token attends to every other token in the sequence, which helps the model capture contextual relationships.<br><br></li>\n",
    "            <div align=\"center\"><img src=\"media/self-attention-exampl.webp\" alt=\"Self-Attention BERT\" width=\"800\"/><br><br></li></div>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><strong>Bidirectionality<br><br></strong>\n",
    "        <ul>\n",
    "            <li>Unlike traditional left-to-right or right-to-left models, BERT reads the entire sequence of words at once. This bidirectional approach allows it to understand the context of a word based on both its left and right surroundings.<br><br></li>\n",
    "            <div align=\"center\"><img src=\"media/bidrectional_example.png\" alt=\"Bidirectional BERT\" width=\"800\"/><br><br></li></div>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><strong>Transformer Layers<br><br></strong>\n",
    "        <ul>\n",
    "            <li>BERT is composed of multiple transformer layers (12 for BERT-base and 24 for BERT-large). Each layer consists of self-attention and feed-forward neural networks.<br><br></li>\n",
    "            <div align=\"center\"><img src=\"media/transformer_layer.png\" alt=\"Transformer Layers BERT\" width=\"800\"/><br><br></li></div>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>\n",
    "\n",
    "</div>\n",
    "\n",
    "*** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### So, how does BERT work?\n",
    "Having covered tokens, special tokens, maximum sequence length, and the absolute basics of **self-attention**. Let's dive into the core of BERT's architecture and training process. \n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"media/bert_architecture.webp\" alt=\"BERT Architecture\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "* Recall that BERT is an **encoder-only** model, meaning it focuses exclusively on understanding the input sequence. In other words BERT is a transformer-based model that focuses exclusively on the encoder component of the original “vanilla” Transformer architecture.\n",
    "\n",
    "* Because BERT is designed for **language understanding** rather than **text generation**, it only needs the encoder. By stacking multiple encoder layers (12 for BERT-Base or 24 for BERT-Large), BERT can learn increasingly complex and context-rich representations of input tokens.\n",
    "\n",
    "* Traditional language models often process text directionally - either left-to-right or right-to-left. This can prevent them from seeing future context when predicting or encoding each token. BERT, however, processes all tokens at once, giving it a **bidirectional** (or “non-directional”) view of the entire sequence.  In practice, this means BERT can “see” both the words before and after a given token, leading to richer context and better performance on tasks like:\n",
    "\n",
    "    - Sentiment Analysis\n",
    "    - Named Entity Recognition (NER)\n",
    "    - Question Answering\n",
    "    - Sentence/Document Classification\n",
    "\n",
    "#### Masked Language Modelling (MLM)\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"media/masked_language_modeling.png\" alt=\"Masked Language Modeling\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "BERT is trained using **Masked Language Modeling** (MLM). This essentially means that we can feed the model with a large corpus of text, mask some of the words, and ask the model to predict the masked words. This process encourages the model to learn contextual relationships between words, as it can’t rely on just the next or previous word to make an educated guess.\n",
    "\n",
    "\n",
    "1. **Random Masking**  \n",
    "   - Before feeding a sentence into BERT, 15% of tokens are randomly replaced with a special `[MASK]` token.  \n",
    "   - Example: “The child came home from **[MASK]**.”\n",
    "\n",
    "2. **Prediction Objective**  \n",
    "   - BERT’s goal is to **recover** the original tokens that were masked.  \n",
    "   - Each token’s vector representation (from the final encoder layer) is passed through a classification head (a feedforward neural net as we covered in class 2) to predict the masked word.\n",
    "\n",
    "3. **Self-Attention with Masks**  \n",
    "   - Because BERT sees the entire sequence (including the `[MASK]` tokens), it uses self-attention to figure out which unmasked tokens can help it guess the masked ones.\n",
    "\n",
    "4. **Loss Function**  \n",
    "   - The model only updates weights based on how accurately it predicts the masked words (ignoring unmasked words).  \n",
    "   - (This can slow down training compared to a unidirectional model, but yields much richer contextual embeddings.)\n",
    "\n",
    "\n",
    "By masking random tokens, BERT is forced to learn relationships between *all* words in a sentence. It can’t simply rely on the next or previous word to make a guess; it has to consider everything else in the sequence. Over time, this (hopefully!) fosters an exceptionally deep understanding of language structure and context.\n",
    "\n",
    "\n",
    "#### Fine-tuning\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"media/bert_classifier.png\" alt=\"Fine-tuning BERT\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "After pre-training on a massive corpus (using MLM and the Next Sentence Prediction objective), BERT is essentially trained to predicted masked word in an input (MLM). However, from here we can **fine-tune** the pre-trained model for various NLP tasks using our own labeled data. This process involves adding a simple classification layer on top of the pre-trained BERT model and training it on a specific task (e.g., sentiment analysis, named entity recognition, etc.). This allows us to leverage BERT’s deep contextual understanding of language for a wide range of NLP tasks.:\n",
    "\n",
    "- **Classification** (e.g., sentiment, topic): Use the final hidden state of `[CLS]` as input to a simple classifier. <br><br>\n",
    "- **Token-Level Tasks** (e.g., NER): Each token’s final hidden state can serve as input to a classifier that assigns labels (e.g., “person,” “location,” etc.).<br><br>\n",
    "- **Question Answering**: Combine the embeddings to find the start and end positions of answers in a passage.<br><br>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. ModernBERT\n",
    "\n",
    "[ModernBERT](https://huggingface.co/docs/transformers/model_doc/modernbert) is a state-of-the-art encoder-only model that builds upon the original BERT architecture. Introduced in December 2024, ModernBERT offers several key improvements over its predecessor, including:\n",
    "\n",
    "- **Increased Sequence Length**: ModernBERT can handle sequences up to 8,192 tokens, compared to BERT’s 512-token limit. This makes it more suitable for long-document tasks and other applications that require processing large amounts of text.<br><br>\n",
    "- **Rotary Positional Embeddings (RoPE)**: ModernBERT uses rotary positional embeddings to encode positional information in the input sequence. This allows the model to capture long-range dependencies more effectively and improves performance on tasks that require understanding of sequential relationships.<br><br>\n",
    "- **Flash Attention**: ModernBERT incorporates Flash Attention, a novel attention mechanism that improves efficiency and reduces computational complexity. Flash Attention is designed to be more memory-efficient than traditional self-attention mechanisms, making it well-suited for large-scale language tasks.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0. Load Data\n",
    "\n",
    "**NOTE**: This will download the dataset from the Hugging Face datasets library. If you've already downloaded it before, it will load from your local cache instead of downloading again. \n",
    "\n",
    "Importantly, this also means that the dataset files are stored on your computer, which can take up space over time. If you need to free up space, you can clear the cache:\n",
    "\n",
    "- **On Mac/Linux**, run:  \n",
    "    ```bash\n",
    "    rm -rf ~/.cache/huggingface/datasets\n",
    "    ```\n",
    "\n",
    "- **On Windows** (Command Prompt), run:\n",
    "\n",
    "    ``` bash\n",
    "    rmdir /s /q %USERPROFILE%\\.cache\\huggingface\\datasets\n",
    "    ```\n",
    "\n",
    "Alternatively, if you don’t want to store the dataset on disk, you can load it directly into memory bu settingg `keep_in_memory=True`:\n",
    "\n",
    "```python\n",
    "ag_news_train = load_dataset(\"fancyzhx/ag_news\", split=\"train[:20%]\", keep_in_memory=True)\n",
    "ag_news_test = load_dataset(\"fancyzhx/ag_news\", split=\"test\", keep_in_memory=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 1 # percent as whole number\n",
    "TEST_SIZE = 10 # percent as whole number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1200\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 760\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ag_news_train = load_dataset(\"fancyzhx/ag_news\", split=f\"train[:{TRAIN_SIZE}%]\", keep_in_memory=True )  # n% of training data\n",
    "ag_news_test = load_dataset(\"fancyzhx/ag_news\", split=f\"test[:{TEST_SIZE}%]\", keep_in_memory=True)  # n% of test data\n",
    "\n",
    "ag_news = DatasetDict({\n",
    "    \"train\": ag_news_train,\n",
    "    \"test\": ag_news_test\n",
    "})\n",
    "\n",
    "ag_news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Load ModernBERT pipeline\n",
    "\n",
    "We can make use for the `pipeline` function from the HuggingFace `Transformers` library to load both the model and tokenizer for ModernBERT. This pipeline will automatically tokenize the input text and prepare it for the model. Note that we don't need to load the model itself yet, just the tokenizer, as we will use the pipeline for tokenization and preprocessing. We indicate that by setting task=`feature-extraction`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "embedder = pipeline(\n",
    "    model=\"answerdotai/ModernBERT-base\",      # model used for embedding\n",
    "    tokenizer=\"answerdotai/ModernBERT-base\",  # tokenizer used for embedding\n",
    "    task=\"feature-extraction\",                # feature extraction task (returns embeddings)\n",
    "    device=0                                  # use GPU 0 if available\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Encode the data\n",
    "\n",
    "In this step, we’ll convert each text in our dataset into a numerical representation that machine learning models can understand. To do this, we use the `ModernBERT` model, which takes in text and produces embeddings — vectors of numbers that capture the meaning of the text.\n",
    "\n",
    "BERT (and ModernBERT) actually creates an embedding for token (each word or subword) in a sentence, but instead of keeping all of them, we’ll extract just one: the embedding of the special `[CLS]` token. As covered in the introduction, this token appears at the beginning of every input and is designed to represent the meaning of the whole sentence. This way, instead of storing a variable number of embeddings for each text, we keep just one fixed-size vector per sentence, making it much easier to work with in machine learning.\n",
    "\n",
    "Note that we process the dataset in batches, meaning we send multiple texts through the model at once. This speeds things up, but it also requires more memory. If you run into memory issues, try reducing the batch size (by adjust the `batch_size` parameter in the `map` function). After this step, each entry in our dataset will have an \"embeddings\" field that contains a single vector. The dataset will now look like this:\n",
    "\n",
    "\n",
    "``` python\n",
    "DatasetDict({\n",
    "    train: Dataset({\n",
    "        features: ['text', 'label', 'embeddings'],\n",
    "        num_rows: 1200\n",
    "    })\n",
    "    test: Dataset({\n",
    "        features: ['text', 'label', 'embeddings'],\n",
    "        num_rows: 760\n",
    "    })\n",
    "})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e191335c37ad4a188fa758f65c6915cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f25cc6f369b2451aaa90d9fb4e641ad0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_embeddings(data):\n",
    "    \"\"\" Extract the [CLS] embedding for each text. \"\"\"\n",
    "    embeddings = embedder(data[\"text\"])  # Full token embeddings\n",
    "    cls_embeddings = [e[0][0] for e in embeddings]  # Extract first token ([CLS])\n",
    "    return {\"embeddings\": cls_embeddings}\n",
    "\n",
    "ag_news = ag_news.map(get_embeddings, batched=True, batch_size=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'embeddings'],\n",
       "        num_rows: 1200\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'embeddings'],\n",
       "        num_rows: 760\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ag_news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can extract features and labels into X_train, y_train, X_test, y_test to fit with the standard scikit-learn paradigm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (1200, 768), y_train shape: (1200,)\n",
      "X_test shape: (760, 768), y_test shape: (760,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train = np.array(ag_news[\"train\"][\"embeddings\"])  # Feature embeddings\n",
    "y_train = np.array(ag_news[\"train\"][\"label\"])       # Labels\n",
    "\n",
    "X_test = np.array(ag_news[\"test\"][\"embeddings\"])\n",
    "y_test = np.array(ag_news[\"test\"][\"label\"])\n",
    "\n",
    "# Check shapes\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Train a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LogisticRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(max_iter=1000)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(max_iter=1000)\n",
    "\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       273\n",
      "           1       1.00      1.00      1.00       182\n",
      "           2       1.00      0.99      1.00       202\n",
      "           3       1.00      1.00      1.00       543\n",
      "\n",
      "    accuracy                           1.00      1200\n",
      "   macro avg       1.00      1.00      1.00      1200\n",
      "weighted avg       1.00      1.00      1.00      1200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = lr.predict(X_train)\n",
    "\n",
    "print(classification_report(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.76      0.75       197\n",
      "           1       0.91      0.87      0.89       199\n",
      "           2       0.66      0.59      0.62       158\n",
      "           3       0.74      0.82      0.78       206\n",
      "\n",
      "    accuracy                           0.77       760\n",
      "   macro avg       0.76      0.76      0.76       760\n",
      "weighted avg       0.77      0.77      0.77       760\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml25-ma2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
