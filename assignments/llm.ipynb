{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AI & Machine Learning (KAN-CINTO4003U) - Copenhagen Business School | Spring 2025**\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Part III: LLM\n",
    "\n",
    "Please see the description of the assignment in the README file (section 3) <br>\n",
    "**Guide notebook**: [guides/llm_guide.ipynb](guides/llm_guide.ipynb)\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "<br>\n",
    "\n",
    "* Note that you should report results using a classification report. \n",
    "\n",
    "* Also, remember to include some reflections on your results: how do they compare with the results from Part I, BoW?, and part II, BERT? Are there any hyperparameters or prompting techniques that are particularly important?\n",
    "\n",
    "* You should follow the steps given in the `llm_guide` notebook\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for the project\n",
    "\n",
    "import pandas as pd;\n",
    "import decouple as config;\n",
    "import os;\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report \n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Watson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| PARAMETER             | TYPE                                   | EXAMPLE VALUE                                                                                                                             |\n",
      "+=======================+========================================+===========================================================================================================================================+\n",
      "| decoding_method       | str, TextGenDecodingMethod, NoneType   | sample                                                                                                                                    |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| length_penalty        | dict, TextGenLengthPenalty, NoneType   | {'decay_factor': 2.5, 'start_index': 5}                                                                                                   |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| temperature           | float, NoneType                        | 0.5                                                                                                                                       |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| top_p                 | float, NoneType                        | 0.2                                                                                                                                       |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| top_k                 | int, NoneType                          | 1                                                                                                                                         |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| random_seed           | int, NoneType                          | 33                                                                                                                                        |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| repetition_penalty    | float, NoneType                        | 2                                                                                                                                         |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| min_new_tokens        | int, NoneType                          | 50                                                                                                                                        |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| max_new_tokens        | int, NoneType                          | 1000                                                                                                                                      |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| stop_sequences        | list, NoneType                         | 200                                                                                                                                       |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| time_limit            | int, NoneType                          | 600000                                                                                                                                    |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| truncate_input_tokens | int, NoneType                          | 200                                                                                                                                       |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| return_options        | dict, ReturnOptionProperties, NoneType | {'input_text': True, 'generated_tokens': True, 'input_tokens': True, 'token_logprobs': True, 'token_ranks': False, 'top_n_tokens': False} |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| include_stop_sequence | bool, NoneType                         | True                                                                                                                                      |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| prompt_variables      | dict, NoneType                         | {'doc_type': 'emails', 'entity_name': 'Golden Retail'}                                                                                    |\n",
      "+-----------------------+----------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "#from decouple import config\n",
    "from ibm_watsonx_ai import APIClient\n",
    "from ibm_watsonx_ai import Credentials\n",
    "from ibm_watsonx_ai.foundation_models import ModelInference\n",
    "\n",
    "from decouple import Config, RepositoryEnv\n",
    "config = Config(RepositoryEnv('.env'))\n",
    "\n",
    "WX_API_KEY = config('WX_API_KEY')\n",
    "credentials = Credentials(\n",
    "    url = \"https://us-south.ml.cloud.ibm.com/\",\n",
    "    api_key = WX_API_KEY\n",
    ")\n",
    "\n",
    "client = APIClient(\n",
    "    credentials=credentials, \n",
    "    project_id=\"3b0687e2-70ee-471b-ba30-6d4514787f00\"\n",
    ")\n",
    "model = ModelInference(\n",
    "    api_client=client,\n",
    "    model_id=\"ibm/granite-13b-instruct-v2\",\n",
    ")\n",
    "\n",
    "from ibm_watsonx_ai.foundation_models.schema import TextGenParameters\n",
    "\n",
    "TextGenParameters.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the data\n",
    "\n",
    "We can load this data directly from [Hugging Face Datasets](https://huggingface.co/docs/datasets/) - The HuggingFace Hub- into a Pandas DataFrame. Pretty neat!\n",
    "\n",
    "**Note**: This cell will download the dataset and keep it in memory. If you run this cell multiple times, it will download the dataset multiple times.\n",
    "\n",
    "You are welcome to increase the `frac` parameter to load more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "splits = {'train': 'data/train-00000-of-00001.parquet', 'test': 'data/test-00000-of-00001.parquet'}\n",
    "# train = pd.read_parquet(\"hf://datasets/fancyzhx/ag_news/\" + splits[\"train\"])\n",
    "test = pd.read_parquet(\"hf://datasets/fancyzhx/ag_news/\" + splits[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((760, 2),)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map = {\n",
    "    0: 'World',\n",
    "    1: 'Sports',\n",
    "    2: 'Business',\n",
    "    3: 'Sci/Tech'\n",
    "}\n",
    "\n",
    "def preprocess(df: pd.DataFrame, frac = 1e-2, label_map = label_map, seed=42) -> pd.DataFrame:\n",
    "    return  (\n",
    "        df\n",
    "        .assign(label=lambda x: x['label'].map(label_map))\n",
    "        [lambda df: df['label'].isin(label_map.values())]\n",
    "        .groupby('label')\n",
    "        .apply(lambda x: x.sample(frac=frac, random_state=seed))\n",
    "        .reset_index(drop=True)\n",
    "\n",
    "    )\n",
    "\n",
    "# train_df = preprocess(train, frac=0.01)\n",
    "test_df = preprocess(test, frac=0.1)\n",
    "\n",
    "# clear up some memory by deleting the original dataframes\n",
    "# del train\n",
    "del test\n",
    "\n",
    "test_df.shape, # train_df.shape, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PARAMS = TextGenParameters(\n",
    "    temperature=0,              # Higher temperature means more randomness - In this case we don't want randomness\n",
    "    max_new_tokens=10,          # Maximum number of tokens to generate\n",
    "    stop_sequences=[\".\", \"\\n\"], # Stop generating text when these sequences are encountered\n",
    ")\n",
    "\n",
    "model = ModelInference(\n",
    "    api_client=client,\n",
    "    model_id=\"ibm/granite-13b-instruct-v2\",  # We could also try a larger model!\n",
    "    params=PARAMS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "System prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# First System Prompt = Basic\n",
    "SYSTEM_PROMPT_1 = \"\"\"You task is to classify news stories into one of five categories\n",
    "\n",
    "CATEGORIES:\n",
    "{categories}\n",
    "\n",
    "TEXT:\n",
    "{text}\n",
    "\n",
    "Please assign the correct category to the text. Answer with the correct category and nothing else.\n",
    "\n",
    "Category:\n",
    "\"\"\"\n",
    "\n",
    "# Second System Prompt = Few-Shot Prompting\n",
    "SYSTEM_PROMPT_2 = \"\"\"You are an expert classifier tasked with labeling news articles into one of the categories: World, Sports, Business, or Sci/Tech. Use the examples below as a guide:\n",
    "\n",
    "EXAMPLES:\n",
    "Example 1:\n",
    "News article: \"Global leaders convened at an international summit to discuss climate change policies.\"\n",
    "Label: World\n",
    "\n",
    "Example 2:\n",
    "News article: \"The local team clinched the championship title after a nail-biting final match.\"\n",
    "Label: Sports\n",
    "\n",
    "Example 3:\n",
    "News article: \"The stock market saw dramatic swings today after the release of the quarterly earnings report.\"\n",
    "Label: Business\n",
    "\n",
    "Example 4:\n",
    "News article: \"Scientists revealed a breakthrough in quantum computing that could transform the tech industry.\"\n",
    "Label: Sci/Tech\n",
    "\n",
    "\n",
    "CATEGORIES:\n",
    "{categories}\n",
    "\n",
    "TEXT:\n",
    "{text}\n",
    "\n",
    "Please assign the correct category to the text. Answer with the correct category and nothing else.\n",
    "\n",
    "Category:\n",
    "\"\"\"\n",
    "\n",
    "# Third System Prompt =  Chain-of-Thought with Guided Reasoning\n",
    "SYSTEM_PROMPT_3 = \"\"\"You are an expert news classifier. For each article, follow this diagnostic reasoning process:\n",
    "\n",
    "1. **Determine the Focus:** Identify the primary focus of the article. Consider if the content primarily addresses political/diplomatic issues, sports events, business/market developments, or technological/scientific advancements.\n",
    "2. **List Supporting Evidence:** Extract and list at least two pieces of evidence (keywords or phrases) from the article that support your interpretation of its focus.\n",
    "3. **Select the Appropriate Label:** Match the evidence to one of the following labels:\n",
    "   - \"World\" for articles on global or international topics.\n",
    "   - \"Sports\" for articles on athletic events or sports.\n",
    "   - \"Business\" for articles on financial news and economic issues.\n",
    "   - \"Sci/Tech\" for articles on technology or scientific discoveries.\n",
    "4. **State Your Final Decision:** Provide category.\n",
    "\n",
    "EXAMPLES:\n",
    "Example 1:\n",
    "News article: \"In a historic summit, world leaders convened to address climate change.\"\n",
    "- Focus Determination: The article covers an international event.\n",
    "- Supporting Evidence: \"historic summit,\" \"world leaders,\" \"climate change.\"\n",
    "- Label Selection: World.\n",
    "- Final Decision: World, because the evidence strongly points to global issues.\n",
    "Category: World\n",
    "\n",
    "Example 2:\n",
    "News article: \"A breakthrough in AI technology promises to reshape the tech industry.\"\n",
    "- Focus Determination: The article centers on technological innovation.\n",
    "- Supporting Evidence: \"breakthrough in AI,\" \"reshape the tech industry.\"\n",
    "- Label Selection: Sci/Tech.\n",
    "- Final Decision: Sci/Tech, as the key evidence revolves around technological advancement.\n",
    "Category: Sci/Tech\n",
    "\n",
    "Now, please classify the following news article by applying the diagnostic reasoning steps above.\n",
    "\n",
    "CATEGORIES:\n",
    "{categories}\n",
    "\n",
    "TEXT:\n",
    "{text}\n",
    "\n",
    "Please assign the correct category to the text. Answer with the correct category and nothing else.\n",
    "\n",
    "Category:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 760/760 [10:53<00:00,  1.16it/s]\n"
     ]
    }
   ],
   "source": [
    "# Ensure CATEGORIES is defined\n",
    "CATEGORIES = 'Business\\nSci/Tech\\nSports\\nWorld'\n",
    "\n",
    "# Store predictions for each system prompt\n",
    "predictions_1 = []\n",
    "predictions_2 = []\n",
    "predictions_3 = []\n",
    "\n",
    "for text in tqdm(test_df[\"text\"]):\n",
    "\n",
    "    # Format and generate response for SYSTEM_PROMPT_1\n",
    "    prompt_1 = SYSTEM_PROMPT_1.format(categories=CATEGORIES, text=text)\n",
    "    response_1 = model.generate(prompt_1)\n",
    "    prediction_1 = response_1[\"results\"][0][\"generated_text\"].strip()\n",
    "    predictions_1.append(prediction_1)\n",
    "\n",
    "    # Format and generate response for SYSTEM_PROMPT_2\n",
    "    prompt_2 = SYSTEM_PROMPT_2.format(categories=CATEGORIES, text=text)\n",
    "    response_2 = model.generate(prompt_2)\n",
    "    prediction_2 = response_2[\"results\"][0][\"generated_text\"].strip()\n",
    "    predictions_2.append(prediction_2)\n",
    "\n",
    "    # Format and generate response for SYSTEM_PROMPT_3\n",
    "    prompt_3 = SYSTEM_PROMPT_3.format(categories=CATEGORIES, text=text)\n",
    "    response_3 = model.generate(prompt_3)\n",
    "    prediction_3 = response_3[\"results\"][0][\"generated_text\"].strip()\n",
    "    predictions_3.append(prediction_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for SYSTEM_PROMPT_1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Business       0.60      0.88      0.71       190\n",
      "    Sci/Tech       0.84      0.58      0.69       190\n",
      "      Sports       0.98      0.92      0.95       190\n",
      "       World       0.84      0.76      0.80       190\n",
      "\n",
      "    accuracy                           0.78       760\n",
      "   macro avg       0.81      0.78      0.79       760\n",
      "weighted avg       0.81      0.78      0.79       760\n",
      "\n",
      "\n",
      "Classification Report for SYSTEM_PROMPT_2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Business       0.47      0.97      0.63       190\n",
      "     Letters       0.00      0.00      0.00         0\n",
      "    Sci/Tech       0.81      0.36      0.50       190\n",
      "      Sports       0.94      0.89      0.92       190\n",
      "       World       0.85      0.46      0.60       190\n",
      "\n",
      "    accuracy                           0.67       760\n",
      "   macro avg       0.61      0.54      0.53       760\n",
      "weighted avg       0.77      0.67      0.66       760\n",
      "\n",
      "\n",
      "Classification Report for SYSTEM_PROMPT_3:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Business       0.40      0.96      0.57       190\n",
      "    Sci/Tech       0.83      0.39      0.54       190\n",
      "      Sports       0.91      0.73      0.81       190\n",
      "       World       0.79      0.27      0.41       190\n",
      "\n",
      "    accuracy                           0.59       760\n",
      "   macro avg       0.73      0.59      0.58       760\n",
      "weighted avg       0.73      0.59      0.58       760\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wwwkr\\anaconda3\\envs\\aiml25-ma2\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\wwwkr\\anaconda3\\envs\\aiml25-ma2\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\wwwkr\\anaconda3\\envs\\aiml25-ma2\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Evaluate performance for SYSTEM_PROMPT_1\n",
    "print(\"Classification Report for SYSTEM_PROMPT_1:\")\n",
    "print(classification_report(test_df.label, predictions_1))\n",
    "\n",
    "# Evaluate performance for SYSTEM_PROMPT_2\n",
    "print(\"\\nClassification Report for SYSTEM_PROMPT_2:\")\n",
    "print(classification_report(test_df.label, predictions_2))\n",
    "\n",
    "# Evaluate performance for SYSTEM_PROMPT_3\n",
    "print(\"\\nClassification Report for SYSTEM_PROMPT_3:\")\n",
    "print(classification_report(test_df.label, predictions_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, SYSTEM_PROMPT_1 performed best, while SYSTEM_PROMPT_3 had the weakest results due to poor recall in multiple categories. It seems that the additional instructions has hindered accuracy scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for SYSTEM_PROMPT_1:\n",
      "          World  Sports  Business  Sci/Tech\n",
      "World       168       9         0        13\n",
      "Sports       70     110         0        10\n",
      "Business     11       1       174         4\n",
      "Sci/Tech     31      11         4       144\n",
      "\n",
      "Overall Test Accuracy for SYSTEM_PROMPT_1: 0.78\n",
      "\n",
      "Per-Class Metrics for SYSTEM_PROMPT_1:\n",
      "World: Precision: 0.60, Recall: 0.88, Accuracy: 0.88\n",
      "Sports: Precision: 0.84, Recall: 0.58, Accuracy: 0.58\n",
      "Business: Precision: 0.98, Recall: 0.92, Accuracy: 0.92\n",
      "Sci/Tech: Precision: 0.84, Recall: 0.76, Accuracy: 0.76\n"
     ]
    }
   ],
   "source": [
    "# Generate confusion matrix for SYSTEM_PROMPT_1\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n",
    "import pandas as pd\n",
    "\n",
    "# Compute the confusion matrix for SYSTEM_PROMPT_1\n",
    "conf_matrix_1 = confusion_matrix(test_df[\"label\"], predictions_1)\n",
    "\n",
    "# Define label map\n",
    "label_map = {\n",
    "    0: 'World',\n",
    "    1: 'Sports',\n",
    "    2: 'Business',\n",
    "    3: 'Sci/Tech'\n",
    "}\n",
    "\n",
    "# Convert the confusion matrix to a DataFrame with descriptive headers\n",
    "conf_matrix_df_1 = pd.DataFrame(\n",
    "    conf_matrix_1,\n",
    "    index=[f\"{label}\" for label in label_map.values()],\n",
    "    columns=[f\"{label}\" for label in label_map.values()]\n",
    ")\n",
    "\n",
    "# Calculate overall test accuracy for SYSTEM_PROMPT_1\n",
    "overall_accuracy_1 = accuracy_score(test_df[\"label\"], predictions_1)\n",
    "\n",
    "# Calculate per-class precision, recall, and accuracy\n",
    "per_class_precision_1 = precision_score(test_df[\"label\"], predictions_1, average=None, zero_division=0)\n",
    "per_class_recall_1 = recall_score(test_df[\"label\"], predictions_1, average=None, zero_division=0)\n",
    "per_class_accuracy_1 = conf_matrix_1.diagonal() / conf_matrix_1.sum(axis=1)\n",
    "\n",
    "# Display the confusion matrix\n",
    "print(\"Confusion Matrix for SYSTEM_PROMPT_1:\")\n",
    "print(conf_matrix_df_1)\n",
    "\n",
    "# Display overall accuracy\n",
    "print(f\"\\nOverall Test Accuracy for SYSTEM_PROMPT_1: {overall_accuracy_1:.2f}\")\n",
    "\n",
    "# Display per-class metrics\n",
    "print(\"\\nPer-Class Metrics for SYSTEM_PROMPT_1:\")\n",
    "for label, precision, recall, accuracy in zip(label_map.values(), per_class_precision_1, per_class_recall_1, per_class_accuracy_1):\n",
    "    print(f\"{label}: Precision: {precision:.2f}, Recall: {recall:.2f}, Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic observations for LLM:\n",
    "\n",
    "1. World:\n",
    "Although \"World\" articles have a high recall (0.88) and high accuracy (0.88), the precision is relatively low (0.60) - considerable number of articles from other categories are being misclassified as \"World.\" In particular, the confusion from other classes (e.g., 70 \"Sports\" and 31 \"Sci/Tech\" instances predicted as \"World\") suggests that the model may be overgeneralizing when content has a global aspect.\n",
    "\n",
    "2. Sports:\n",
    "\"Sports\" articles show a high precision (0.84), meaning that when the model predicts an article as \"Sports,\" it is usually correct. However, the recall is low (0.58), which reveals that a significant portion of actual \"Sports\" articles are not being recognized as such—70 instances of \"Sports\" articles are incorrectly classified as \"World.\" This misclassification indicates that the model might be conflating some sports content with broader global news.\n",
    "\n",
    "3. Business:\n",
    "The \"Business\" category exhibits the strongest performance, with an impressive precision (0.98) and recall (0.92), resulting in the highest accuracy (0.92) among the categories. This suggests that the vocabulary and features present in business news are highly distinctive, allowing the model to accurately differentiate them from other categories with minimal confusion.\n",
    "\n",
    "4. Sci/Tech:\n",
    "\"Sci/Tech\" articles have a balanced performance with a precision of 0.84 and a recall of 0.76, leading to an accuracy of 0.76. However, the model still confuses some Sci/Tech content with \"World\" (31 instances) and \"Sports\" (11 instances) articles. This indicates that while the model performs reasonably well in identifying technological and scientific content, there remains some overlap with other categories, particularly when the subject matter spans global or sports-related contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final remarks\n",
    "\n",
    "* The BoW model had noticeable struggles with distinguishing between similar categories, particularly confusing \"World\" with \"Sports\" and \"Sci/Tech.\" but it has the highest accuracy amongst all the models.\n",
    "* Pre-trained LLMs can match BERT in performance if prompted effectively, reducing the need for computationally expensive fine-tuning.\n",
    "* LLM prompt nr 1, has outperformed the others because it likely provided clearer instructions and better context."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml25-ma2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
